---
title: "Class 8 Machine Learning"
author: "Reina Bassil"
date: "10/25/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means example

We will make up some data to cluster

```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))

plot(x)
```



Use the kmeans() function setting k to 2 and nstart=20

```{r}
k <- kmeans(x, centers = 2, nstart = 20)
```

Inspect/print the results
```{r}
k
```


Q. How many points are in each cluster?

```{r}
k$size
```

Q. What ‘component’ of your result object details
 - cluster size?
 - cluster assignment/membership?
 - cluster center?
 
Plot x colored by the kmeans cluster assignment and
 add cluster centers as blue points
 
```{r}
table(k$cluster)
```


```{r}
plot(x, col= k$cluster)
points(k$centers, col = "blue", pch = 15)
```



## Hierarchical clustering in R


```{r}
hc <- hclust( dist(x))
hc
```

Plot my results
```{r}
plot(hc)
abline(h=6, col="red")
abline(h=4, col="blue")
#h is height, not that same as k 
cutree(hc, h=6)
```


```{r}
grps <- cutree(hc, h=4)
table(grps)
```


I can also cut the tree to yield a given "k" groups/clusters


```{r}
cutree(hc, k=2)

cutree(hc, k=4)
```


```{r}
grps <- cutree(hc, k=2)

plot(x, col=grps)
```




```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
```


```{r}
# Step 2. Plot the data without clustering
plot(x)

```


```{r}
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```


```{r}
ex <- hclust( dist(x))
plot(ex)
```

```{r}
grps2 <- cutree(ex,k=2)

grps3 <- cutree(ex, k=3)

plot(x, col=grps2)

plot(x, col=grps3)

plot(x, col=col)
```


```{r}
table(col, grps3)
```


## Principle Component Analysis

How to use the prcomp() function to do PCA.
• How to draw and interpret PCA plots
• How to determine how much variation each principal
component accounts for and the the “intrinsic
dimensionality” useful for further analysis
• How to examine the loadings (or loading scores) to
determine what variables have the largest effect on the
graph and are thus important for discriminating samples.
```{r}
mydata <- read.csv("https://tinyurl.com/expression-CSV",
 row.names=1)
head(mydata)

mydata2 <- read.csv("https://tinyurl.com/expression-CSV")
head(mydata2)
```


Now we have our data we call prcomp() to do PCA
• NOTE: prcomp() expects the samples to be rows and
genes to be columns so we need to first transpose the
matrix with the t() function!

```{r}
pca <- prcomp(t(mydata), scale=TRUE) 

## See what is returnedby the prcomp() function
attributes(pca)
```



```{r}
pca$x[,1]
```


Plot PCA1 vs. PCA2 

```{r}
plot(pca$x[,1], pca$x[,2])
```


```{r}
summary(pca)
```


Scree Plotting

```{r}
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

barplot(pca.var.per, main="Scree Plot",
 xlab="Principal Component", ylab="Percent Variation")
```


Can also just plot PCA and it does the calculation for you, but it's important to know where the calculation comes from - square of the standard deviation 
```{r}
plot(pca)
```



```{r}
plot(pca$x[,1:2], col= c())


```




```{r}
rep("red", 5)

rep("blue", 5)
```



## Practice

```{r}
x <- read.csv("UK_foods.csv")
```

Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?
```{r}
dim(x)
```

Checking your data

```{r}
head(x)
```


Hmm, it looks like the row-names here were not set properly as we were expecting 4 columns (one for each of the 4 countries of the UK - not 5 as reported from the dim() function).

Here it appears that the row-names are incorrectly set as the first column of our x data frame (rather than set as proper row-names). This is very common error. Lets try to fix this up with the following code, which sets the rownames() to the first column and then removes the troublesome first column (with the -1 column index):

```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```


Check new dimensions
```{r}
dim(x)
```

Side-note: An alternative approach to setting the correct row-names in this case would be to read the data filie again and this time set the row.names argument of read.csv() to be the first column (i.e. use argument setting row.names=1), see below:

```{r}
x <- read.csv("UK_foods.csv", row.names=1)
head(x)
```

```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

Q3: Changing what optional argument in the above barplot() function results in the following plot?

```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```



Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

```{r}
pairs(x, col=rainbow(10), pch=16)
```



As we noted in the lecture portion of class, prcomp() expects the observations to be rows and the variables to be columns therefore we need to first transpose our data.frame matrix with the t() transpose function.



```{r}
pca <- prcomp( t(x) )
summary(pca)
```


Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points.

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```


Q8. Customize your plot so that the colors of the country names match the colors in our UK and Ireland map and table at start of this document.





## Digging deeper (variable loadings)


```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )

```














